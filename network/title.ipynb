{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "list=[]\n",
    "f= open('title.csv','r',encoding=\"utf-8\")\n",
    "csv_reader = csv.reader(f)\n",
    "for row in csv_reader:\n",
    "    list.append(row[0])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(2351 unique tokens: [\"'Brazilian style science' - an analysis of the difference between Brazilian and international Computer Science departments and graduate programs using social networks analysis and bibliometrics.\", \"'It could turn ugly': Selective disclosure of attitudes in political discussion networks.\", \"'It must be me' or 'It could be them?': The impact of the social network position of bullies and victims on victims\\\\' adjustment.\", \"'Think before you upload': an in-depth analysis of unavailable videos on YouTube.\", \"'Trust Me': Differences in expressed and perceived trust relations in an organization.\"]...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary([list])#方法遍历所有的文本，为每个不重复的单词分配一个单独的整数ID\n",
    "print(dictionary)#[' ',' ',' '...]12220\n",
    "\n",
    "#Dictionary 函数的补充\n",
    "#dictionary.add_documents([['',''],['']])#更新词典，加入新的不重复词\n",
    "\n",
    "#dictionary.doc2bow(['','',''])#输出：[(词ID，词频)..]，对该文本进行词袋格式的转换，查找词典在文档出现的词和次数进行输出\n",
    "\n",
    "#dictionary.doc2idx(['','',''])#输出：[对应词在词典中的索引号，，]，不在词典中的词的索引号可通过属性unknow_word_index设置\n",
    "\n",
    "#https://radimrehurek.com/gensim/corpora/dictionary.html更多有关函数可参考官网\n",
    "\n",
    "#将词汇表写入文本vocab.txt中\n",
    "f = open('vocab.txt','w')\n",
    "#l为索引号\n",
    "for l in dictionary:\n",
    "    #print(dictionary[l])\n",
    "    f.write(dictionary[l]+'\\n')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(42, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow([doc]) for doc in list]\n",
    "#第13个标题向量的结果\n",
    "print(corpus[12])#[(5, 1), (64, 1), (66, 1), (67, 1), (68, 1), (69, 2), (70, 1)] len(corpus[12])=7\n",
    "\n",
    "#将结果按格式要求存入文本文件title.txt\n",
    "#ti代表一条标题向量，len(ti)计算该向量的项数\n",
    "f = open(\"title.txt\",\"w\")\n",
    "for ti in corpus:\n",
    "   #print(len(ti))\n",
    "   f.write(str(len(ti))+' ')\n",
    "   for i in range(len(ti)):\n",
    "      # print(str(ti[i][0])+\":\"+str(ti[i][1]))\n",
    "       f.write(str(ti[i][0])+\":\"+str(ti[i][1])+\" \")\n",
    "   f.write(\"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f91969b55433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#用模型训练新文档，分配主题\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mother_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mother_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mtoken2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# 在 DT 矩阵上运行和训练 LDA 模型\n",
    "model = gensim.models.ldamodel.LdaModel(corpus, num_topics=5)#指定主题数为5\n",
    "\n",
    "\"\"\"\n",
    "#将模型保存到磁盘或重新加载预先训练的模型\n",
    "temp_file='/users/sds'\n",
    "model.save(temp_file)\n",
    "model=Ldamodel.load(temp_file)\n",
    "\"\"\"\n",
    "\n",
    "#用模型训练新文档，分配主题\n",
    "other_text=[['','','']]\n",
    "other_corpus=dictionary.doc2bow([other_text])\n",
    "vector=model[other_corpus]\n",
    "\n",
    "#对新语料库进行增量培训来更新模型\n",
    "model.update(other_corpus)\n",
    "\n",
    "#获取单个主题及主题下的单词\n",
    "print_topic(topicno,topn=k)#k为显示的单词数\n",
    "print_topics(num_topics=n,num_words=k)#k同上，n为显示的主题数\n",
    "\n",
    "#获取给定文档的主题分布\n",
    "get_document_topics(bow,minimum_probability)#bow为词袋格式\n",
    "\n",
    "corpus_lda = model[corpus]  # 每行标题对应5个主题的权重\n",
    "print(len(corpus_lda))#30796行标题\n",
    "print(corpus_lda[0])#[(0, 0.48325282), (1, 0.04041054), (2, 0.040259134), (3, 0.39596453), (4, 0.040113024)]\n",
    "\n",
    "#获取每个单词的主题分布，每个词对应5个主题的权重,取权重最高的为其主题，存入字典d1\n",
    "d1=dict()\n",
    "#print(d1)\n",
    "for l in dictionary:\n",
    "    term = model.get_term_topics(l, minimum_probability=1e-8)#设置minimum_probability阈值，选出大于阈值的主题，输出[(主题编号，权重)...]\n",
    "    i_max = term[0][1]\n",
    "    label = 0\n",
    "    for i in range(1, len(term)):\n",
    "        #print(term[i][1])\n",
    "        if (term[i][1] > i_max):\n",
    "            i_max = term[i][1]\n",
    "            label = i\n",
    "    #print(dictionary[l]+\" \"+str(label))\n",
    "    d1[dictionary[l]]=label\n",
    "print(d1)#[(词，主题编号),()...]\n",
    "\n",
    "#遍历paper.txt，每行属于同一个主题的分配到一个文件topic-i.txt中\n",
    "for list1 in list:\n",
    "    print(list1)\n",
    "    for j in range(len(list1)):\n",
    "        #print(list1[j])\n",
    "        te=list1[j]\n",
    "       # print(d1[list1[j]])\n",
    "        l=d1[list1[j]]\n",
    "        f=open(\"topic-\"+str(l)+\".txt\",\"a\")\n",
    "        f.write(te+\" \")\n",
    "        f.close()\n",
    "    for k in range(0,5):\n",
    "         f=open(\"topic-\"+str(k)+\".txt\",\"a\")\n",
    "         f.write(\"\\n\")\n",
    "         f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"topic-4.txt\")#以最后一个主题文件为例\n",
    "\n",
    "line = f.readline()\n",
    "list = []\n",
    "while line:\n",
    "    a = line.split()\n",
    "    b = a[0:]\n",
    "    if b==[]:\n",
    "        line = f.readline()\n",
    "        continue\n",
    "    list.append(b)\n",
    "    line = f.readline()\n",
    "#print(list):[[],[],[]...]\n",
    "f.close()\n",
    "\n",
    "#转换为DataFrame格式\n",
    "import pandas as pd\n",
    "\n",
    "t_df = pd.DataFrame(list)\n",
    "#print(t_df):18345*9,18345行中最长的一行有9个单词\n",
    "\n",
    "#转换DataFrame数据为包含数据的列表\n",
    "# df_arr = shopping_df.stack().groupby(level=0).apply(list).tolist()   # 方法一\n",
    "def deal(data):\n",
    "    return data.dropna().tolist()\n",
    "df_arr = t_df.apply(deal, axis=1).tolist()  # 方法二\n",
    "#print(df_arr)：[[],[],[]...]\n",
    "\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder  # 传入模型的数据需要满足特定的格式，可以用这种方法来转换为bool值\n",
    "\n",
    "te = TransactionEncoder()  # 定义模型\n",
    "df_tf = te.fit_transform(df_arr)\n",
    "df = pd.DataFrame(df_tf, columns=te.columns_)\n",
    "#print(df):18345*2364，一共18345行，2364个单词\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=0.002, use_colnames=True)  # use_colnames=True表示使用元素名字，默认的False使用列名代表元素\n",
    "frequent_itemsets.sort_values(by='support', ascending=False, inplace=True)  # 频繁项集可以按支持度排序\n",
    "print(frequent_itemsets)  # 打印频繁项集\n",
    "frequent_itemsets.to_csv('G://pattern-4.txt', sep='\\t', index=False, header=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
